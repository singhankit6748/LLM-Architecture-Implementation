{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b68787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imported Libraries ##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afef30fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:\tcuda\n"
     ]
    }
   ],
   "source": [
    "### Config ###\n",
    "\n",
    "GPT2_CONFIG = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_blocks\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        }\n",
    "\n",
    "GPT2_CONFIG[\"ff_hidden_size\"] = 4 * GPT2_CONFIG[\"emb_dim\"]\n",
    "\n",
    "print(f\"device:\\t{GPT2_CONFIG['device']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962370cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens shape:\ttorch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "### tokenizer ###\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"My name is Ankit \"\n",
    "enc_inp = tokenizer.encode(text, return_tensors='pt').to(GPT2_CONFIG[\"device\"])\n",
    "print(f\"input tokens shape:\\t{enc_inp.shape}\")\n",
    "\n",
    "''' Embedding '''\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size=GPT2_CONFIG[\"vocab_size\"], emb_dim=GPT2_CONFIG[\"emb_dim\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.emb(x)\n",
    "\n",
    "        print(f\"embedded tokens shape:\\t{out.shape}\")\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732645ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Positional Encoding ###\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len=GPT2_CONFIG[\"context_length\"], emb_dim=GPT2_CONFIG[\"emb_dim\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, emb_dim, device=device)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim)).to(device)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.pe[:x.size(1), :]\n",
    "\n",
    "        assert x.shape == out.shape, \"input embedding shape does not match positional encoding shape\"\n",
    "        print(f\"positional encoding shape:\\t{out.shape}\")\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caac2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masked Attention Head ###\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, emb_dim=GPT2_CONFIG[\"emb_dim\"], qkv_bias=GPT2_CONFIG[\"qkv_bias\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.qw = nn.Linear(emb_dim, head_size, bias=qkv_bias, device=device)\n",
    "        self.kw = nn.Linear(emb_dim, head_size, bias=qkv_bias, device=device)\n",
    "        self.vw = nn.Linear(emb_dim, head_size, bias=qkv_bias, device=device)\n",
    "\n",
    "    def forward(self, x, device=GPT2_CONFIG[\"device\"]):\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        mask = torch.triu(torch.full((x.size(1), x.size(1)), float('-inf'), device=device), diagonal=1)\n",
    "\n",
    "        qk = q @ k.transpose(-2, -1)\n",
    "        scaling = qk * (self.emb_dim ** -0.5)\n",
    "        add_mask = scaling + mask\n",
    "        scaled_sm = F.softmax(add_mask, dim=-1)\n",
    "        qk_v = scaled_sm @ v\n",
    "\n",
    "        print(f\"head shape:\\t{qk_v.shape}\")\n",
    "        return qk_v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff90e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi Head Masked Attention ###\n",
    "\n",
    "class Multi_Head(nn.Module):\n",
    "    def __init__(self, emb_dim=GPT2_CONFIG[\"emb_dim\"], n_heads=GPT2_CONFIG[\"n_heads\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = emb_dim // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(self.head_size) for _ in range(n_heads)])\n",
    "        self.lyr = nn.Linear(emb_dim, emb_dim, bias=False, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.lyr(out)\n",
    "\n",
    "        assert x.shape == out.shape, \"positional encoding input shape does not match multi-head output shape\"\n",
    "        print(f\"multi head shape:\\t{out.shape}\")\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e4ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feed Forward Layer ###\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim=GPT2_CONFIG[\"emb_dim\"], ff_hidden_size=GPT2_CONFIG[\"ff_hidden_size\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lyr_1 = nn.Linear(emb_dim, ff_hidden_size, device=device)\n",
    "        self.lyr_2 = nn.Linear(ff_hidden_size, emb_dim, device=device)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.lyr_1(x)\n",
    "        out = self.gelu(out)\n",
    "        out = self.lyr_2(out)\n",
    "\n",
    "        print(f\"feed forward shape:\\t{out.shape}\")\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fd51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer Block ###\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim=GPT2_CONFIG[\"emb_dim\"], drop_rate=GPT2_CONFIG[\"drop_rate\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.m_head = Multi_Head()\n",
    "        self.ff = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln1(x + self.dropout(self.m_head(x)))\n",
    "        out = self.ln2(out + self.dropout(self.ff(out)))\n",
    "\n",
    "        print(f\"block shape:\\t{out.shape}\")\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8261a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT2 Model Class ###\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, n_blocks=GPT2_CONFIG[\"n_blocks\"], emb_dim=GPT2_CONFIG[\"emb_dim\"], vocab_size=GPT2_CONFIG[\"vocab_size\"], device=GPT2_CONFIG[\"device\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embd = Embedding()\n",
    "        self.pos_e = PositionalEncoding()\n",
    "\n",
    "        self.block_list = nn.ModuleList([Block() for _ in range(n_blocks)])\n",
    "        self.lyr = nn.Linear(emb_dim, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pos_e(self.embd(x))\n",
    "\n",
    "        for block in self.block_list:\n",
    "            out = block(out)\n",
    "\n",
    "        out = self.lyr(out)\n",
    "\n",
    "        print(f\"GPT2 shape:\\t{out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0601c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded tokens shape:\ttorch.Size([1, 6, 768])\n",
      "positional encoding shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "head shape:\ttorch.Size([1, 6, 64])\n",
      "multi head shape:\ttorch.Size([1, 6, 768])\n",
      "feed forward shape:\ttorch.Size([1, 6, 768])\n",
      "block shape:\ttorch.Size([1, 6, 768])\n",
      "GPT2 shape:\ttorch.Size([1, 6, 50257])\n",
      "GPT2 Params:\t162262609\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2()\n",
    "gpt2.to(GPT2_CONFIG[\"device\"])\n",
    "x_gpt2 = gpt2(enc_inp)\n",
    "\n",
    "gpt2_params = sum(p.numel() for p in gpt2.parameters())\n",
    "print(f\"GPT2 Params:\\t{gpt2_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afe508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
